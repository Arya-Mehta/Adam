# Custom Adam Optimizer in PyTorch

## Overview

This repository contains a custom implementation of the Adam optimizer from scratch in PyTorch. Adam (Adaptive Moment Estimation) is one of the most widely used optimization algorithms in deep learning due to its adaptive learning rate capabilities. The implementation closely follows the concepts introduced in the original research paper while integrating it into the PyTorch framework for seamless compatibility.

## Features

- **Custom Adam Optimizer**: Implemented entirely from scratch, following the mathematical foundations of the Adam optimizer algorithm.
- **Datasets Used**: Tested on two datasets from SciKit:
  - Breast Cancer Dataset
  - Digit Dataset
- **Performance Comparison**: Benchmarked against PyTorch's built-in optimizers:
  - Stochastic Gradient Descent (SGD)
  - Inbuilt Adam Optimizer
- **Results**: Space for detailed comparison results (accuracy, training loss, and convergence speed).

